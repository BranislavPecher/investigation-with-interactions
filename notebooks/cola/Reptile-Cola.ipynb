{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0243df3-6b28-4b35-821c-80b99ea544ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ac4ff-ce67-421d-ac25-678409a31ac6",
   "metadata": {},
   "source": [
    "# Reptile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e666637c",
   "metadata": {},
   "source": [
    "Note that the code below implements loading of data from new experiments. We also provide pickled data from running our experiments -- loading of this data is at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e51034a-dc0d-4dfa-b9eb-baa61f47fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Reptile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641fc74-ba0d-4c9a-8a8c-862072890690",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('..', '..', 'results', 'stability', 'cola')\n",
    "FACTORS = os.path.join(DATA_PATH, 'factors', 'predictions')\n",
    "GOLDEN = os.path.join(DATA_PATH, 'golden', 'predictions', 'golden_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa87a1ba-8d1b-44d9-9220-890debc26a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed = 0\n",
    "all = 0 \n",
    "\n",
    "for split in os.listdir(GOLDEN):\n",
    "    split_path = os.path.join(GOLDEN, split)\n",
    "    for label in os.listdir(split_path):\n",
    "        label_path = os.path.join(split_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for run in os.listdir(label_path):\n",
    "                run_path = os.path.join(label_path, run)\n",
    "                if os.path.isdir(run_path):\n",
    "                    for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                        evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                        if os.path.isdir(evaluation_path):\n",
    "                            with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                data = json.load(file)\n",
    "                            score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                            results.append(score)\n",
    "                            if score < 0.5:\n",
    "                                failed += 1\n",
    "                            all += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec18f11-745c-49e7-9b0e-aa68b5beffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.167725115980986"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(results) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e8d03c7-e0af-4adb-a5c4-afde38077884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.501107030589822"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(results) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df50f5a-8faa-47cc-8809-4348ac19f698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 13.81%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {failed / all * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01223a-2375-4260-bf42-735fc8dc1ddf",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40e6229-18e4-40f5-94ea-c2715633ad87",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e940ff4-2ed9-454f-8a4b-24fb06998121",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(FACTORS, 'data_split')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for run in os.listdir(label_path):\n",
    "                    run_path = os.path.join(label_path, run)\n",
    "                    if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                        for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                            evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                            if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                    data = json.load(file)\n",
    "                                score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                split_number = int(split.split('_')[1])\n",
    "                                label_number = int(label.split('_')[1])\n",
    "                                run_number = int(run.split('_')[1])\n",
    "                                adaptation_number = int(evaluation.split('_')[1])\n",
    "                                \n",
    "                                data_split_results['results'].append({\n",
    "                                    'score': score,\n",
    "                                    'split': split_number,\n",
    "                                    'label': label_number,\n",
    "                                    'run': run_number,\n",
    "                                    'adaptation': adaptation_number\n",
    "                                })\n",
    "                                if score < 0.5:\n",
    "                                    data_split_results['failed'] += 1\n",
    "                                data_split_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986933ef-ef4c-4525-9c8c-e8f9d9b5f967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1411)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_split_results['results']), data_split_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969fdff-6b78-405a-b7fb-0e0fe8c98c5b",
   "metadata": {},
   "source": [
    "#### Label Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd824752-6d01-488c-8fa3-5f590ceb54d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(FACTORS, 'label_selection')\n",
    "                                \n",
    "for label in os.listdir(factor_path):\n",
    "    label_path = os.path.join(factor_path, label)\n",
    "    if label.startswith('label_') and os.path.isdir(label_path):\n",
    "        for split in os.listdir(label_path):\n",
    "            split_path = os.path.join(label_path, split)\n",
    "            if split.startswith('split_') and os.path.isdir(split_path):\n",
    "                for run in os.listdir(split_path):\n",
    "                    run_path = os.path.join(split_path, run)\n",
    "                    if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                        for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                            evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                            if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                    data = json.load(file)\n",
    "                                score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                split_number = int(split.split('_')[1])\n",
    "                                label_number = int(label.split('_')[1])\n",
    "                                run_number = int(run.split('_')[1])\n",
    "                                adaptation_number = int(evaluation.split('_')[1])\n",
    "                                \n",
    "                                label_results['results'].append({\n",
    "                                    'score': score,\n",
    "                                    'split': split_number,\n",
    "                                    'label': label_number,\n",
    "                                    'run': run_number,\n",
    "                                    'adaptation': adaptation_number\n",
    "                                })\n",
    "                                if score < 0.5:\n",
    "                                    label_results['failed'] += 1\n",
    "                                label_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffbe5a3a-9413-4d54-9acc-36f987d0aa8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1645)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_results['results']), label_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f96fb2-212f-4d90-b854-a3aa5a61ee23",
   "metadata": {},
   "source": [
    "#### Choice of Adaptation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83926646-8db1-40ac-971a-67308a9515d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(FACTORS, 'model_adaptation')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for run in os.listdir(label_path):\n",
    "                    run_path = os.path.join(label_path, run)\n",
    "                    if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                        for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                            evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                            if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                    data = json.load(file)\n",
    "                                score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                split_number = int(split.split('_')[1])\n",
    "                                label_number = int(label.split('_')[1])\n",
    "                                run_number = int(run.split('_')[1])\n",
    "                                adaptation_number = int(evaluation.split('_')[1])\n",
    "                                \n",
    "                                adaptation_results['results'].append({\n",
    "                                    'score': score,\n",
    "                                    'split': split_number,\n",
    "                                    'label': label_number,\n",
    "                                    'run': run_number,\n",
    "                                    'adaptation': adaptation_number\n",
    "                                })\n",
    "                                if score < 0.5:\n",
    "                                    adaptation_results['failed'] += 1\n",
    "                                adaptation_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea08052f-360d-47e5-977c-d6ebe0500e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9722)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adaptation_results['results']), adaptation_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98134f8-5356-4aa6-ac6d-234932654481",
   "metadata": {},
   "source": [
    "##### Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b88b78b-ff59-4c2e-a310-4efe4e5bbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_adaptation_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(DATA_PATH, 'adaptation_stable', 'predictions', 'model_adaptation')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for run in os.listdir(label_path):\n",
    "                    run_path = os.path.join(label_path, run)\n",
    "                    if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                        for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                            evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                            if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                    data = json.load(file)\n",
    "                                score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                split_number = int(split.split('_')[1])\n",
    "                                label_number = int(label.split('_')[1])\n",
    "                                run_number = int(run.split('_')[1])\n",
    "                                adaptation_number = int(evaluation.split('_')[1])\n",
    "                                \n",
    "                                stable_adaptation_results['results'].append({\n",
    "                                    'score': score,\n",
    "                                    'split': split_number,\n",
    "                                    'label': label_number,\n",
    "                                    'run': run_number,\n",
    "                                    'adaptation': adaptation_number\n",
    "                                })\n",
    "                                if score < 0.5:\n",
    "                                    stable_adaptation_results['failed'] += 1\n",
    "                                stable_adaptation_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0627f53f-2367-4235-bd7a-ae8f69aed6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2687)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stable_adaptation_results['results']), stable_adaptation_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77d65e-bee2-43b7-83c1-cf610103a534",
   "metadata": {},
   "source": [
    "##### Unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8988f43d-4688-45ac-91d1-c94adde16f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_adaptation_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(DATA_PATH, 'adaptation_unstable', 'predictions', 'model_adaptation')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for run in os.listdir(label_path):\n",
    "                    run_path = os.path.join(label_path, run)\n",
    "                    if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                        for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                            evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                            if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                    try:\n",
    "                                        data = json.load(file)\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                split_number = int(split.split('_')[1])\n",
    "                                label_number = int(label.split('_')[1])\n",
    "                                run_number = int(run.split('_')[1])\n",
    "                                adaptation_number = int(evaluation.split('_')[1])\n",
    "                                \n",
    "                                unstable_adaptation_results['results'].append({\n",
    "                                    'score': score,\n",
    "                                    'split': split_number,\n",
    "                                    'label': label_number,\n",
    "                                    'run': run_number,\n",
    "                                    'adaptation': adaptation_number\n",
    "                                })\n",
    "                                if score < 0.5:\n",
    "                                    unstable_adaptation_results['failed'] += 1\n",
    "                                unstable_adaptation_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7481e57-4fda-4075-af2e-e3b59bb2d539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 96152)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unstable_adaptation_results['results']), unstable_adaptation_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceefb20-1522-4174-a9e9-afe06d8b93b4",
   "metadata": {},
   "source": [
    "#### Initialisation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504b7014-165f-47f0-9abd-0e8fa9af4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialisation_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(FACTORS, 'model_initialisation')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for initialisation in os.listdir(label_path):\n",
    "                    initialisation_path = os.path.join(label_path, initialisation)\n",
    "                    if initialisation.startswith('init_') and os.path.isdir(initialisation_path):\n",
    "                        for run in os.listdir(initialisation_path):\n",
    "                            run_path = os.path.join(initialisation_path, run)\n",
    "                            if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                                for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                                    evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                                    if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                        with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                            data = json.load(file)\n",
    "                                        score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                        split_number = int(split.split('_')[1])\n",
    "                                        label_number = int(label.split('_')[1])\n",
    "                                        run_number = int(run.split('_')[1])\n",
    "                                        adaptation_number = int(evaluation.split('_')[1])\n",
    "                                        initialisation_number = int(initialisation.split('_')[1])\n",
    "                                \n",
    "                                        initialisation_results['results'].append({\n",
    "                                            'score': score,\n",
    "                                            'split': split_number,\n",
    "                                            'label': label_number,\n",
    "                                            'run': run_number,\n",
    "                                            'adaptation': adaptation_number,\n",
    "                                            'initialisation': initialisation_number\n",
    "                                        })\n",
    "                                        if score < 0.5:\n",
    "                                            initialisation_results['failed'] += 1\n",
    "                                        initialisation_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "139f980c-e47d-4dbc-8dbb-a56602d48fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2832)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initialisation_results['results']), initialisation_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4e50c-d9f7-47cd-9bd9-16ddc912a02a",
   "metadata": {},
   "source": [
    "#### Order of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0ef37a2-39a2-42b4-acec-3b329fa1c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_results = {\n",
    "    'results': [],\n",
    "    'failed': 0,\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "factor_path = os.path.join(FACTORS, 'data_order')\n",
    "\n",
    "for split in os.listdir(factor_path):\n",
    "    split_path = os.path.join(factor_path, split)\n",
    "    if split.startswith('split_') and os.path.isdir(split_path):\n",
    "        for label in os.listdir(split_path):\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if label.startswith('label_') and os.path.isdir(label_path):\n",
    "                for order in os.listdir(label_path):\n",
    "                    order_path = os.path.join(label_path, order)\n",
    "                    if order.startswith('data_order_') and os.path.isdir(order_path):\n",
    "                        for run in os.listdir(order_path):\n",
    "                            run_path = os.path.join(order_path, run)\n",
    "                            if run.startswith('run_') and os.path.isdir(run_path) and os.path.exists(os.path.join(run_path, model_name)):  \n",
    "                                for evaluation in os.listdir(os.path.join(run_path, model_name)):\n",
    "                                    evaluation_path = os.path.join(run_path, model_name, evaluation)\n",
    "                                    if evaluation.startswith('evaluation_') and os.path.isdir(evaluation_path):\n",
    "                                        with open(os.path.join(evaluation_path, 'results.json'), 'r') as file:\n",
    "                                            data = json.load(file)\n",
    "                                        score = f1_score(np.array(data['predictions'][0]), np.array(data['predictions'][1]), average='macro')\n",
    "                                \n",
    "                                        split_number = int(split.split('_')[1])\n",
    "                                        label_number = int(label.split('_')[1])\n",
    "                                        run_number = int(run.split('_')[1])\n",
    "                                        adaptation_number = int(evaluation.split('_')[1])\n",
    "                                        order_number = int(order.split('_')[2])\n",
    "                                \n",
    "                                        order_results['results'].append({\n",
    "                                            'score': score,\n",
    "                                            'split': split_number,\n",
    "                                            'label': label_number,\n",
    "                                            'run': run_number,\n",
    "                                            'adaptation': adaptation_number,\n",
    "                                            'order': order_number\n",
    "                                        })\n",
    "                                        if score < 0.5:\n",
    "                                            order_results['failed'] += 1\n",
    "                                        order_results['all'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c150f2af-c4c1-4864-ba6a-22d00bbaf420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1213)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(order_results['results']), order_results['failed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6d142-3673-4a56-8e6c-9a7a73a0a578",
   "metadata": {},
   "source": [
    "## Compare Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84346a-7c28-47e4-b3da-4a82f1325c62",
   "metadata": {},
   "source": [
    "### Aggregation by investigated factor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "685c7e49-b650-45cf-8bba-3412427ca1dc",
   "metadata": {},
   "source": [
    "In this part we use the aggreagtion by the main investigated factor in following way:\n",
    "- select runs where the value of factors only differ in the investigated factor (non-investigated factors have the same value; investigated has 10 values)\n",
    "- calculate mean and standard deviation across the values of investigated factor\n",
    "- results in ~10 000 values of mean and standard deviation\n",
    "- calculate the final performance values as a mean of the pre-calculated many mean values\n",
    "- calculate the instability of factor by calculating mean of the pre-calculated standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045887c8-b934-4a3a-98f6-584f12cda564",
   "metadata": {},
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "591be841-e0f3-402e-ad01-80dd806d32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.44589397698361, 10.240922173953663, 22.878970647366305, 66.11793179361038)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in data_split_results['results']:\n",
    "    # key = f\"split_{value['split']}-label_{value['label']}-run_{value['run']}-adaptation_{value['adaptation']}\"\n",
    "    key = f\"label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    score = result['score'] * 100\n",
    "    if data_split_by_other_factors.get(key, None) is None:\n",
    "        data_split_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        data_split_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d722fb71-9783-46cd-b374-4fa25b576d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 56.445893976983605\n",
      "Investigated factor deviation: 8.549640070907888\n",
      "Other factors deviation: 3.1749591603067815\n",
      "Variability of factor deviation: 4.658301832351148\n"
     ]
    }
   ],
   "source": [
    "aggregated_data_split = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in data_split_by_other_factors.items():\n",
    "    aggregated_data_split['mean'].append(np.mean(factor_value))\n",
    "    aggregated_data_split['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_data_split['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_data_split['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_data_split['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_data_split['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48a7c371-9dd5-450b-b3dc-b873a8474c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 14.11%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {data_split_results['failed'] / data_split_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049a5ef-1e1e-4fae-9ac6-a1591392b43f",
   "metadata": {},
   "source": [
    "#### Label Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2af14ac-01ff-4a4c-a493-c9fb477f848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.15618759447901, 11.076961711167472, 22.909967845659164, 65.78783450286805)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in label_results['results']:\n",
    "    # key = f\"split_{value['split']}-label_{value['label']}-run_{value['run']}-adaptation_{value['adaptation']}\"\n",
    "    key = f\"split_{result['split']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    score = result['score'] * 100\n",
    "    if label_by_other_factors.get(key, None) is None:\n",
    "        label_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        label_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cf0b3dc-c4be-47e5-9d0c-dd248555fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 56.156187594479015\n",
      "Investigated factor deviation: 9.482023299061868\n",
      "Other factors deviation: 3.3975413408742807\n",
      "Variability of factor deviation: 4.609449830919988\n"
     ]
    }
   ],
   "source": [
    "aggregated_label = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in label_by_other_factors.items():\n",
    "    aggregated_label['mean'].append(np.mean(factor_value))\n",
    "    aggregated_label['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_label['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_label['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_label['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_label['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65ed1266-57a0-4c70-8684-a302d8a86f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 16.45%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {label_results['failed'] / label_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04a6b3-014e-41d5-babd-d64bdbfb3423",
   "metadata": {},
   "source": [
    "#### Choice of Adaptation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8de9e37-abe5-4e8f-8413-5b7f70774709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.00167380894774, 9.790735846128984, 22.75473217881595, 53.73433918894539)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptation_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in adaptation_results['results']:\n",
    "    # key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}\"\n",
    "    score = result['score'] * 100\n",
    "    if adaptation_by_other_factors.get(key, None) is None:\n",
    "        adaptation_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        adaptation_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c103574-2e3a-480e-958b-bde92712b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 35.00167380894774\n",
      "Investigated factor deviation: 9.036489861529297\n",
      "Other factors deviation: 3.47799031879031\n",
      "Variability of factor deviation: 1.4504974090132399\n"
     ]
    }
   ],
   "source": [
    "aggregated_adaptation = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in adaptation_by_other_factors.items():\n",
    "    aggregated_adaptation['mean'].append(np.mean(factor_value))\n",
    "    aggregated_adaptation['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_adaptation['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_adaptation['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_adaptation['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_adaptation['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0469dfbb-ffc2-4fcc-835c-22aa1d03a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 97.22%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {adaptation_results['failed'] / adaptation_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29ffaa-a362-47ea-86c3-e6cc962d1a0a",
   "metadata": {},
   "source": [
    "##### Stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "953120ec-c4aa-455d-a2ea-9641b2d19297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60.00161994794071, 4.968874220054435, 22.878970647366305, 66.25381812949264)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_adaptation_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in stable_adaptation_results['results']:\n",
    "    # key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}\"\n",
    "    score = result['score'] * 100\n",
    "    if stable_adaptation_by_other_factors.get(key, None) is None:\n",
    "        stable_adaptation_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        stable_adaptation_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "daed0539-c731-420d-ac5e-9b1d6fd21bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 60.001619947940696\n",
      "Investigated factor deviation: 2.5103252701732757\n",
      "Other factors deviation: 2.171166490481539\n",
      "Variability of factor deviation: 3.6978391153836205\n"
     ]
    }
   ],
   "source": [
    "aggregated_stable_adaptation = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in stable_adaptation_by_other_factors.items():\n",
    "    aggregated_stable_adaptation['mean'].append(np.mean(factor_value))\n",
    "    aggregated_stable_adaptation['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_stable_adaptation['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_stable_adaptation['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_stable_adaptation['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_stable_adaptation['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "152d690c-3ece-4333-83d2-baadaf6971bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 2.6870000000000003%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {stable_adaptation_results['failed'] / stable_adaptation_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d96c0-79b5-4f61-897e-2d9881e3d82f",
   "metadata": {},
   "source": [
    "##### Unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d96e8f09-9f47-406f-a7a3-4c4bc78b73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.551894438690994, 10.012034385556646, 22.630092779346516, 55.59678884145763)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unstable_adaptation_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in unstable_adaptation_results['results']:\n",
    "    # key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}\"\n",
    "    score = result['score'] * 100\n",
    "    if unstable_adaptation_by_other_factors.get(key, None) is None:\n",
    "        unstable_adaptation_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        unstable_adaptation_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52bfcba2-e04c-4840-8f8d-e8ca36a71232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 35.551894438691\n",
      "Investigated factor deviation: 9.843380132014717\n",
      "Other factors deviation: 1.7259314328018474\n",
      "Variability of factor deviation: 0.6081618234427127\n"
     ]
    }
   ],
   "source": [
    "aggregated_unstable_adaptation = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in unstable_adaptation_by_other_factors.items():\n",
    "    aggregated_unstable_adaptation['mean'].append(np.mean(factor_value))\n",
    "    aggregated_unstable_adaptation['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_unstable_adaptation['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_unstable_adaptation['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_unstable_adaptation['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_unstable_adaptation['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c351a233-7e33-4267-82ba-6851b1c2eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 96.152%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {unstable_adaptation_results['failed'] / unstable_adaptation_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a6bb0-2a63-46e7-8f60-85363a9de5ce",
   "metadata": {},
   "source": [
    "#### Initialisation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "baaecf1f-5b6f-401f-8721-626ad7fca003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56.86936996662, 10.650003357050839, 22.909967845659164, 65.93044505307995)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialisation_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in initialisation_results['results']:\n",
    "    key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    score = result['score'] * 100\n",
    "    if initialisation_by_other_factors.get(key, None) is None:\n",
    "        initialisation_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        initialisation_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e39eabc6-30c1-4a59-ab06-a6430c3ddfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 56.86936996662\n",
      "Investigated factor deviation: 8.325033983520006\n",
      "Other factors deviation: 3.9786815455383167\n",
      "Variability of factor deviation: 5.318502969598116\n"
     ]
    }
   ],
   "source": [
    "aggregated_initialisation = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in initialisation_by_other_factors.items():\n",
    "    aggregated_initialisation['mean'].append(np.mean(factor_value))\n",
    "    aggregated_initialisation['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_initialisation['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_initialisation['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_initialisation['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_initialisation['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d3ec0b1-b051-4213-a275-b1b12de81892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 14.16%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {initialisation_results['failed'] / initialisation_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6d553-866f-478c-8a4e-57fc1102df1f",
   "metadata": {},
   "source": [
    "#### Order of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05460a24-bc6b-4b53-a724-fee9a721675f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.171245357227875, 7.33660323456349, 22.878970647366305, 66.49344340651919)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_by_other_factors = {}\n",
    "overall_score = []\n",
    "\n",
    "for result in order_results['results']:\n",
    "    key = f\"split_{result['split']}-label_{result['label']}-run_{result['run']}-adaptation_{result['adaptation']}\"\n",
    "    score = result['score'] * 100\n",
    "    if order_by_other_factors.get(key, None) is None:\n",
    "        order_by_other_factors[key] = [score]\n",
    "    else:\n",
    "        order_by_other_factors[key].append(score)\n",
    "    overall_score.append(score)\n",
    "np.mean(overall_score), np.std(overall_score), np.min(overall_score), np.max(overall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da7825eb-0543-4916-a4b2-593f11986b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigated factor mean: 59.17124535722787\n",
      "Investigated factor deviation: 4.689745835801505\n",
      "Other factors deviation: 2.9592420180661727\n",
      "Variability of factor deviation: 4.803635882901779\n"
     ]
    }
   ],
   "source": [
    "aggregated_order = {'mean': [], 'std': []}\n",
    "\n",
    "for _, factor_value in order_by_other_factors.items():\n",
    "    aggregated_order['mean'].append(np.mean(factor_value))\n",
    "    aggregated_order['std'].append(np.std(factor_value))\n",
    "\n",
    "print(f\"Investigated factor mean: {np.mean(aggregated_order['mean'])}\")\n",
    "print(f\"Investigated factor deviation: {np.mean(aggregated_order['std'])}\")\n",
    "print(f\"Other factors deviation: {np.std(aggregated_order['mean'])}\")\n",
    "print(f\"Variability of factor deviation: {np.std(aggregated_order['std'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7f7fbdc0-4ac8-4a15-935d-88316012439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed percentage of runs: 6.065%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed percentage of runs: {order_results['failed'] / order_results['all'] * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bda13-ff9c-4989-8831-695ef0180302",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c58e3aec-0900-4516-8d73-f5ddf12fb889",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_PATH = os.path.join('..', '..', 'pickle', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dab0ac9c-8380-47fb-a1f1-54ebb4740cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, 'Reptile-data'), 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'golden': results,\n",
    "        'split': data_split_results,\n",
    "        'label': label_results,\n",
    "        'initialisation': initialisation_results,\n",
    "        'order': order_results,\n",
    "        'adaptation': adaptation_results,\n",
    "        's_adaptation': stable_adaptation_results,\n",
    "        'u_adaptation': unstable_adaptation_results,\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02dac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PICKLE_PATH, 'Reptile-data'), 'rb') as file:\n",
    "    pickled = pickle.load(file)\n",
    "\n",
    "results = pickled['golden']\n",
    "data_split_results = pickled['split']\n",
    "label_results = pickled['label']\n",
    "initialisation_results = pickled['initialisation']\n",
    "order_results = pickled['order']\n",
    "adaptation_results = pickled['adaptation_results']\n",
    "stable_adaptation_results = pickled['s_adaptation']\n",
    "unstable_adaptation_results = pickled['u_adaptation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_PROJECT",
   "language": "python",
   "name": "nlp_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
